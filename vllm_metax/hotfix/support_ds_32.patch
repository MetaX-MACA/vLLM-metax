diff --git a/vllm/config/compilation.py b/vllm/config/compilation.py
index 5a3517362..d73c60dc5 100644
--- a/vllm/config/compilation.py
+++ b/vllm/config/compilation.py
@@ -501,7 +501,7 @@ class CompilationConfig:
         "vllm::plamo2_mamba_mixer",
         "vllm::gdn_attention_core",
         "vllm::kda_attention",
-        "vllm::sparse_attn_indexer",
+        "vllm::mx_sparse_attn_indexer",
     ]
 
     def compute_hash(self) -> str:
diff --git a/vllm/v1/spec_decode/eagle.py b/vllm/v1/spec_decode/eagle.py
index 5bf2503c3..8b1917f63 100644
--- a/vllm/v1/spec_decode/eagle.py
+++ b/vllm/v1/spec_decode/eagle.py
@@ -20,7 +20,7 @@ from vllm.logger import init_logger
 from vllm.model_executor.layers.attention_layer_base import AttentionLayerBase
 from vllm.model_executor.model_loader import get_model
 from vllm.model_executor.models import supports_multimodal
-from vllm.model_executor.models.deepseek_v2 import DeepseekV32IndexerCache
+from vllm_metax.models.deepseek_v2 import DeepseekV32IndexerCache
 from vllm.model_executor.models.llama_eagle3 import Eagle3LlamaForCausalLM
 from vllm.multimodal import MULTIMODAL_REGISTRY
 from vllm.platforms import current_platform
diff --git a/vllm/v1/worker/utils.py b/vllm/v1/worker/utils.py
index 095407a8b..33356d25b 100644
--- a/vllm/v1/worker/utils.py
+++ b/vllm/v1/worker/utils.py
@@ -321,6 +321,8 @@ def bind_kv_cache(
                 # case. Some test code depends on runner_kv_caches, but
                 # not in a way that's impacted by ignoring this.
                 pass
+            elif current_platform.is_out_of_tree():
+                pass
             else:
                 raise NotImplementedError
         layer_name = layer_names[0]
