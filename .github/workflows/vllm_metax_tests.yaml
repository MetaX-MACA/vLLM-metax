name: "test"

on:
  workflow_dispatch:
  push:
    branches:
      - "master"
  pull_request:
    branches:
      - "master"

defaults:
  run:
    shell: bash -el {0}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  changes:
    name: Detect Changed Paths
    runs-on: ubuntu-latest
    outputs:
      base_tracker: ${{ steps.filter.outputs.base_tracker }}
      attn_backend_tracker: ${{ steps.filter.outputs.attn_backend_tracker }}
      quantization_tracker: ${{ steps.filter.outputs.quantization_tracker }}
      ut_tracker: ${{ steps.filter.outputs.ut_tracker }}
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            base_tracker:
              - '.github/workflows/vllm_metax_tests.yaml'
              - 'vllm_metax/**'
              - 'csrc/**'
              - 'cmake/**'
              - 'CMakeLists.txt'
              - 'setup.py'
              - 'requirements/**'
            attn_backend_tracker:
              - 'vllm_metax/v1/attention/backends/**'
              - 'vllm_metax/backends/**'
            quantization_tracker:
              - 'vllm_metax/quant_config'
            ut_tracker:
              - 'tests/ut/**'

  e2e_test:
    needs: [changes]
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request' && needs.changes.outputs.base_tracker == 'true'}}
    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        os: [vllm-metax-runner-set]
        vllm_version: [v0.10.2, main]
    runs-on: ${{ matrix.os }}
    steps:
      - name: Install base packages
        run: |
          sudo apt-get update -y
          sudo apt install git -y
          conda activate base
          pip install uv

      - name: Validate GPU/Env/Permissions/Groups
        run: |
          echo "==== GPU Device Permission ===="
          ls -l /dev/dri* || echo "No /dev/dri* found"
          echo "==== User Info ===="
          id
          whoami
          echo "==== User Groups ===="
          groups
          getent group video || echo "No 'video' group found"
          getent group docker || echo "No 'docker' group found"
          echo "==== Env Vars ===="
          echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"
          echo "PATH=$PATH"
          echo "LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
          echo "==== Python CUDA/Torch ===="
          python -c "import torch; print('torch.cuda.is_available:', torch.cuda.is_available())"

      - name: Checkout vllm repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          repository: vllm-project/vllm
          ref: ${{ matrix.vllm_version }}
          path: ./vllm-official

      - name: Extra setup for vllm
        if: ${{ matrix.vllm_version == 'v0.10.2' }}
        working-directory: ./vllm-official
        run: |
          rm use_existing_torch.py
          wget -nv https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/use_existing_torch.py

      - name: Install vllm
        working-directory: ./vllm-official
        run: |
          echo "$PATH"
          conda activate base
          python use_existing_torch.py > /dev/null
          pip install -r requirements/build.txt
          VLLM_TARGET_DEVICE=empty pip install . --no-build-isolation

      - name: Checkout vLLM-metax repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Ensure cuda toolkit 11.6
        run: |
          if [ ! -d "/usr/local/cuda" ]; then
            echo "CUDA not found, installing..."
            wget -nv https://developer.download.nvidia.com/compute/cuda/11.6.0/local_installers/cuda_11.6.0_510.39.01_linux.run
            sudo sh cuda_11.6.0_510.39.01_linux.run --silent --toolkit
            rm cuda_11.6.0_510.39.01_linux.run
          else
            echo "CUDA already installed, skipping installation."
          fi

      - name: Install vLLM-metax
        run: |
          export MACA_PATH="/opt/maca"
          export CUDA_PATH="/usr/local/cuda"
          export CUCC_PATH="${MACA_PATH}/tools/cu-bridge"
          export PATH="${CUDA_PATH}/bin:${MACA_PATH}/mxgpu_llvm/bin:${MACA_PATH}/bin:${CUCC_PATH}/tools:${CUCC_PATH}/bin:${PATH}"
          export LD_LIBRARY_PATH="${MACA_PATH}/lib:${MACA_PATH}/ompi/lib:${MACA_PATH}/mxgpu_llvm/lib:${LD_LIBRARY_PATH}"
          export VLLM_INSTALL_PUNICA_KERNELS=1

          conda activate base
          pip install -r requirements/build.txt
          pip install . --no-build-isolation -v

      - name: Run basic tests
        if: ${{needs.changes.outputs.base_tracker == 'true'}}
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
          VLLM_MODEL_REDIRECT_PATH: ".modelpath/cicd.json"
        run: |
          # -------------- e2e -----------------------
          pytest tests/e2e/test_offline_inference_basic.py

          # -------------- unit tests -----------------------
          #TODO: add unit tests

      - name: Run attention backend tests
        if: ${{needs.changes.outputs.attn_backend_tracker == 'true'}}
        env:
          VLLM_WORKER_MULTIPROC_METHOD: spawn
          VLLM_MODEL_REDIRECT_PATH: ".modelpath/cicd.json"
        run: |
          # -------------- e2e -----------------------
          pytest tests/e2e/test_offline_inference_attn_backend.py

          # -------------- unit tests -----------------------
          pytest -sv tests/kernels/attention/test_attention_selector.py

      - name: Collect environment info
        if: failure()
        run: |
          sudo bash -c '
          mkdir -p /tmp/collect_env
          cd /tmp/collect_env
          wget -nv https://raw.githubusercontent.com/MetaX-MACA/vLLM-metax/refs/heads/master/vllm_metax/collect_env.py
          # For security purposes, please feel free to check the contents of collect_env.py before running it.
          python collect_env.py
          '
